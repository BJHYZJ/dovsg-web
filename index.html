<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation">
  <meta name="keywords" content="3D scene graph, Long-term Tasks, Mobile Manipulation, Open vocabulary">
  <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- Our site title and description -->
  <link rel="shortcut icon" href="static/images/favicon.ico" type="image/x-icon" />
  <title>DovSG</title>

  <script>

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-fullhd">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a target="_blank" href="https://bjhyzj.github.io/">Zhijie Yan</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://scholar.google.com.hk/citations?user=CpCQmkwAAAAJ&hl=en">Shufei Li</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://scholar.google.com.sg/citations?user=kja7k5MAAAAJ&hl=en">Zuoxu Wang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://scholar.google.com/citations?user=ziAzfCoAAAAJ&hl=en">Lixiu Wu</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Han Wang</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Jun Zhu</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Lijiang Chen</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Jihong Liu</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Beihang University,</span>
              <span class="author-block"><sup>2</sup>Nanyang Technological University,</span>
              <span class="author-block"><sup>3</sup>Minzu University of China,</span>
              <span class="author-block"><sup>4</sup>Afanti Tech LLC</span>
            </div>

            <br>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv Link. -->
                <span class="link-block">
                  <a target="_blank" href="http://arxiv.org/abs/2410.11989" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Video Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://youtu.be/xmUCHzE6EYc"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>

                <!-- Code Link. -->
                <span class="link-block">
                  <a target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span><a href="https://github.com/BJHYZJ/DovSG">Code</a></span>
                  </a>
                </span>

              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Enabling mobile robots to perform long-term tasks in dynamic real-world environments is a formidable challenge, especially when the environment changes frequently due to human-robot interactions or the robot's own actions. Traditional methods typically assume static scenes, which limits their applicability in the continuously changing real world.
              To overcome these limitations, we present DovSG, a novel mobile manipulation framework that leverages dynamic open-vocabulary 3D scene graphs and a language-guided task planning module for long-term task execution. 
              DovSG takes RGB-D sequences as input and utilizes vision-language models (VLMs) for object detection to obtain high-level object semantic features. Based on the segmented objects, a structured 3D scene graph is generated for low-level spatial relationships. Furthermore, an efficient mechanism for locally updating the scene graph, allows the robot to adjust parts of the graph dynamically during interactions without the need for full scene reconstruction.  
              This mechanism is particularly valuable in dynamic environments, enabling the robot to continually adapt to scene changes and effectively support the execution of long-term tasks. 
              We validated our system in real-world environments with varying degrees of manual modifications, demonstrating its effectiveness and superior performance in long-term tasks.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <!-- Contributions Section -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Contributions</h2>
          <div class="content has-text-justified">
            <ul>
              <li>We propose a novel robotic framework that integrates dynamic open-vocabulary 3D scene graphs with language-guided task planning, enabling accurate long-term task execution in dynamic and interactive environments.</li>
              <li>We construct dynamic 3D scene graphs that capture rich object semantics and spatial relations, performing localized updates as the robot interacts with its environment, allowing it to adapt efficiently to incremental modifications.</li>
              <li>We develop a task planning method that decomposes complex tasks into manageable subtasks, including pick-up, place, and navigation, enhancing the robotâ€™s flexibility and scalability in long-term missions.</li>
              <li>We implement DovSG on real-world mobile robots and demonstrate its capabilities across dynamic environments, showing excellent performance in both long-term tasks and subtasks like navigation and manipulation.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <hr>

  <section class="hero teaser" style="position: relative; top: 0px">
    <div class="container is-fullhd">
      <div class="hero-body">
        <div class="container">
          <div class="columns is-vcentered  is-centered">
            <video id="teaser_video" controls muted autoplay loop height="80%" width="80%">
              <source src="media/videos/dovsg_show_method.mp4" type="video/mp4">
          </div>
          <h2 class="subtitle has-text-centered" style="position: relative; top: 0px">
            <span class="dperact">DovSG</span> constructs a <b>Dynamic 3D Scene Graph</b> and leverages <b>task decomposition with large language models</b>,
            enabling <b>localized updates</b> of the 3D scene graphs during <b>interactive exploration</b>. 
            This assists <b>mobile robots</b> in accurately executing <b>long-term tasks</b>, even in scenarios where human modifications to the environment are present.
        </div>
      </div>
    </div>
  </section>

  <hr>

  <section class="section">
    <div class="container is-max-widescreen">

      <div class="rows">
        <h2 class="title is-3">Demos</h2>

        <p class="content has-text-justified">
          Our DovSG enables long-term task execution by constructing dynamic scene graphs 
          and leveraging large language models, even in scenarios where human interactions alter the environment.
        </p>

        <div class="columns">
          <div class="column has-text-centered">
            <video id="dist1" controls muted autoplay loop width="99%">
              <source src="media/videos/dovsg_demo_1.mp4" type="video/mp4">
            </video>
            <p style="text-align:center">
              <b>Minor Adjustment</b>: Please move the red pepper to the plate, then move the green pepper to plate.
            </p>
          </div>

          <div class="column has-text-centered">
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/dovsg_demo_2.mp4" type="video/mp4">
            </video>
            <b>Appearance</b>: Grab the blue toy and place it to the green toy, then move the green pepper to plate.
          </div>
        </div>
      </div>
  </section>



  <section class="section">
    <div class="container is-max-widescreen">

      <div class="rows">


        <!-- Animation. -->
        <div class="rows is-centered ">
          <div class="row is-full-width">
            </br>
            </br>

            <h2 class="title is-3"><span class="dperact">Overview of Our DovSG System.</span></h2>

            <!-- Interpolating. -->
            <div class="content has-text-justified">
              <!-- <br> -->
            </div>
            <div style="text-align: center;">
              <img src="media/figures/teaser.png" class="interpolation-image" width="100%" height="100%" />
            </div>
            <!-- </br>
            </br> -->
            <p class="content has-text-justified">
              DovSG is a mobile robotic system designed to perform long-term tasks in real-world environments. 
              It can detect changes in the scene during task execution, ensuring that subsequent subtasks are completed correctly. 
              The system consists of five main components: perception, memory, task planning, navigation, and manipulation. 
              The memory module includes a lower-level semantic memory and a higher-level scene graph, both of which are continuously updated as the robot explores the environment. 
              This enables the robot to promptly detect manual changes (e.g., keys being moved from cabinet to table) and make the necessary adjustments for subsequent tasks (such as correctly executing Task 2-2).
            </p>
          </div>
        </div>
  </section>



  <section class="section">
    <div class="container is-max-widescreen">

      <div class="rows">


        <!-- Animation. -->
        <div class="rows is-centered ">
          <div class="row is-full-width">
            </br>
            </br>

            <h2 class="title is-3"><span class="dperact">Initialization and Construction of 3D Scene Graphs</span></h2>

            <!-- Interpolating. -->
            <div class="content has-text-justified">
              <!-- <br> -->
            </div>
            <div style="text-align: center;">
              <img src="media/figures/3d_scene_graph_generation.png" class="interpolation-image" width="100%" height="100%" />
            </div>
            <!-- </br>
            </br> -->
            <p class="content has-text-justified">
              We first use the RGB-D-based DROID-SLAM\cite{teed2021droid} model to predict the pose of each frame in the scene. 
              Then, we apply an advanced Open-Vocal segmentation model to segment regions in the RGB images, 
              extract semantic feature vectors for each region, and project them onto a 3D point cloud. 
              Based on semantic, geometric, and CLIP feature similarities, the same object captured from multiple views is gradually associated and fused, 
              resulting in a series of 3D objects. Next, we infer the relationships between objects based on their spatial positions and generate edges connecting these objects, 
              forming a scene graph. This scene graph provides a structured and comprehensive understanding of the scene, 
              allowing efficient localization of target objects and enabling easy reconstruction and updating in dynamic environments, and supports task planning for large language models.
            </p>
          </div>
        </div>
  </section>


  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
  
        <div class="column is-half">
          <h2 class="title is-3">
            <span class="dperact">Adaptation in interactions with manually modified scenes.</span>
          </h2>
          <div class="content has-text-justified">
          </div>
          <div style="text-align: center;">
            <img src="media/figures/relocalization_and_update.png" class="interpolation-image" width="100%" height="100%" />
          </div>
          <p class="content has-text-justified">
            <b>(1)</b> We train the scene-specific regression MLP of the ACE model using RGB images and their poses, making the process highly efficient.
            <b>(2)</b> After manual scene modification, multi-view observations allow rough global pose estimation via ACE, refined further using LightGlue and ICP.
            The new viewpointâ€™s point cloud closely aligns with the stored pose.
            <b>(3)</b> The bottom image shows accurate local updates to the scene based on observations from the new viewpoint.
          </p>
        </div>
  
        <div class="column is-half">
          <h2 class="title is-3">
            <span class="dperact">Two proposed grasp strategies in DovSG.</span>
          </h2>
          <div class="content has-text-justified">
          </div>
          <div style="text-align: center;">
            <img src="media/figures/pickup_strategy.png" class="interpolation-image" width="100%" height="100%" />
          </div>
          <p class="content has-text-justified">
            In the first row, we cropped the point cloud input into anyGrasp within a certain range around the target object, allowing anyGrasp to focus more on the target object without compromising the generation of collision-free grasps.
            Furthermore, we filtered the grasps based on translational and rotational costs, with the red grasps indicating the highest confidence.
            In the second row, we show our heuristic grasp strategy, which leverages the object's bounding box information to rotate and select the most appropriate grasp.
          </p>
        </div>
  
      </div>
    </div>
  </section>


  <hr>

  <section class="section">
    <div class="container is-max-widescreen">

      <div class="rows">
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/xmUCHzE6EYc" frameborder="0" allow="autoplay; encrypted-media"
            allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{yan2024dynamicopenvocabulary3dscene,
        title={Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation}, 
        author={Zhijie Yan and Shufei Li and Zuoxu Wang and Lixiu Wu and Han Wang and Jun Zhu and Lijiang Chen and Jihong Liu},
        year={2024},
        eprint={2410.11989},
        archivePrefix={arXiv},
        primaryClass={cs.RO},
        url={https://arxiv.org/abs/2410.11989}, 
  }</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-centered">
            <p>
              Send feedback and questions to <a href="yanzhijie@buaa.edu.cn">Zhijie Yan</a>. 
              Last updated on October 23, 2024.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>


</body>

</html>