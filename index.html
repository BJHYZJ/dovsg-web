<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation">
  <meta name="keywords" content="3D scene graph, Long-term Tasks, Mobile Manipulation, Open vocabulary">
  <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- Our site title and description -->
  <link rel="shortcut icon" href="static/images/favicon.ico" type="image/x-icon" />
  <title>DovSG</title>

  <script>

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <section class="section">
    <div class="hero-body">
      <div class="container is-fullhd">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a target="_blank" href="https://bjhyzj.github.io/">Zhijie Yan</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://scholar.google.com.hk/citations?user=CpCQmkwAAAAJ&hl=en">Shufei Li</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://scholar.google.com.sg/citations?user=kja7k5MAAAAJ&hl=en">Zuoxu Wang</a><sup>1ðŸ“§</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://scholar.google.com/citations?user=ziAzfCoAAAAJ&hl=en">Lixiu Wu</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Han Wang</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Jun Zhu</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Lijiang Chen</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Jihong Liu</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Beihang University,</span>
              <span class="author-block"><sup>2</sup>Nanyang Technological University,</span>
              <span class="author-block"><sup>3</sup>Minzu University of China,</span>
              <span class="author-block"><sup>4</sup>Afanti Tech LLC</span>
            </div>

            <br>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv Link. -->
                <span class="link-block">
                  <a target="_blank" href="http://arxiv.org/abs/2410.11989" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file"></i>
                    </span>
                    <span><b>arXiv</b></span>
                  </a>
                </span>

                <!-- Video Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://youtu.be/xmUCHzE6EYc"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span><b>Video</b></span>
                  </a>
                </span>

                <!-- Code Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://github.com/BJHYZJ/DovSG"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span><b>Code</b></span>
                  </a>
                </span>

              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" style="position: relative; padding: 0px;">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Enabling mobile robots to perform long-term tasks in dynamic real-world environments is a formidable challenge, especially when the environment changes frequently due to human-robot interactions or the robot's own actions. Traditional methods typically assume static scenes, which limits their applicability in the continuously changing real world.
              To overcome these limitations, we present DovSG, a novel mobile manipulation framework that leverages dynamic open-vocabulary 3D scene graphs and a language-guided task planning module for long-term task execution. 
              DovSG takes RGB-D sequences as input and utilizes vision-language models (VLMs) for object detection to obtain high-level object semantic features. Based on the segmented objects, a structured 3D scene graph is generated for low-level spatial relationships. Furthermore, an efficient mechanism for locally updating the scene graph, allows the robot to adjust parts of the graph dynamically during interactions without the need for full scene reconstruction.  
              This mechanism is particularly valuable in dynamic environments, enabling the robot to continually adapt to scene changes and effectively support the execution of long-term tasks. 
              We validated our system in real-world environments with varying degrees of manual modifications, demonstrating its effectiveness and superior performance in long-term tasks.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" style="position: relative;">
    <div class="container is-max-widescreen">

      <div class="rows">


        <!-- Animation. -->
        <div class="rows is-centered ">
          <div class="row is-full-width">
            </br>
            </br>

            <!-- <h2 class="title is-3"><span class="dperact">Overview of Our DovSG System.</span></h2> -->

            <!-- Interpolating. -->
            <div class="content has-text-justified">
              <!-- <br> -->
            </div>
            <div style="text-align: center;">
              <img src="media/figures/teaser.png" class="interpolation-image" width="80%" height="100%" />
            </div>
            <!-- </br>
            </br> -->
            <p class="content has-text-justified">
              <b>DovSG</b> is a <b>mobile robotic system</b> designed to perform long-term tasks in <b>real-world environments</b>. 
              It can detect changes in the scene during task execution, ensuring that subsequent subtasks are completed correctly. 
              The system consists of five main components: <b>perception</b>, <b>memory</b>, <b>task planning</b>, <b>navigation</b>, and <b>manipulation</b>. 
              The <b>memory module</b> includes a lower-level <b>semantic memory</b> and a higher-level <b>scene graph</b>, both of which are continuously updated as the robot explores the environment. 
              This enables the robot to promptly detect manual changes (e.g., keys being moved from <b>cabinet</b> to <b>table</b>) and make the necessary adjustments for subsequent tasks (such as correctly executing <b>Task 2-2</b>).
            </p>
            
          </div>
        </div>
  </section>



  <section class="section" style="position: relative;">
    <div class="container is-fullhd">
      <div class="hero-body">
        <div class="container">
          <div class="columns is-vcentered  is-centered">
            <video id="teaser_video" controls muted autoplay loop height="100%" width="70%">
              <source src="media/videos/dovsg_show_method.mp4" type="video/mp4">
          </div>
          <h2 class="subtitle has-text-centered" style="position: relative; top: 0px">
            <span class="dperact">DovSG</span> constructs a <b>Dynamic 3D Scene Graph</b> and leverages <b>task decomposition with large language models</b>,
            enabling <b>localized updates</b> of the 3D scene graphs during <b>interactive exploration</b>. 
            This assists <b>mobile robots</b> in accurately executing <b>long-term tasks</b>, even in scenarios where human modifications to the environment are present.
        </div>
      </div>
    </div>
  </section>



  <section class="section" style="position: relative;">
    <div class="container is-max-desktop">
      <!-- Contributions Section -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Contributions</h2>
          <div class="content has-text-justified">
            <ul>
              <li>We propose a novel robotic framework that integrates dynamic open-vocabulary 3D scene graphs with language-guided task planning, enabling accurate long-term task execution in dynamic and interactive environments.</li>
              <li>We construct dynamic 3D scene graphs that capture rich object semantics and spatial relations, performing localized updates as the robot interacts with its environment, allowing it to adapt efficiently to incremental modifications.</li>
              <li>We develop a task planning method that decomposes complex tasks into manageable subtasks, including pick-up, place, and navigation, enhancing the robotâ€™s flexibility and scalability in long-term missions.</li>
              <li>We implement DovSG on real-world mobile robots and demonstrate its capabilities across dynamic environments, showing excellent performance in both long-term tasks and subtasks like navigation and manipulation.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" style="position: relative;">
    <div class="container is-max-widescreen">

      <div class="rows">
        <h2 class="title is-3">Demos</h2>

        <p class="content has-text-justified">
          Our DovSG enables long-term task execution by constructing dynamic scene graphs 
          and leveraging large language models, even in scenarios where human interactions alter the environment.
        </p>

        <div class="columns">
          <div class="column has-text-centered">
            <video id="dist1" controls muted autoplay loop width="99%">
              <source src="media/videos/dovsg_demo_1.mp4" type="video/mp4">
            </video>
            <p style="text-align:center">
              <b>Minor Adjustment</b>: Please move the <u style="color: red;">red pepper</u> to the <u style="font-family: 800;">plate</u>, then move the <u style="color: green;">green pepper</u> to <u>plate</u>.
            </p>
          </div>

          <div class="column has-text-centered">
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/dovsg_demo_2.mp4" type="video/mp4">
            </video>
            <b>Appearance</b>: Grab the <u style="color: blue;">blue toy</u> and place it to the <u style="color: green;">green toy</u>, then move the <u style="color: green;">green pepper</u> to <u>plate</u>.
          </div>
        </div>

        <div class="columns">
          <div class="column has-text-centered">
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/video_1.mp4" type="video/mp4">
            </video>
            <b>Minor Adjustment</b>: Move the <u style="color: green;">green pepper</u> to the <u>plate</u>, then throw the <u style="color: red;">Coca-Cola bottle</u> into the <u>trash can</u>.
          </div>

          <div class="column has-text-centered">
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/video_2.mp4" type="video/mp4">
            </video>
            <b>Appearance</b>: Please move the <u style="color: purple;">eggplant</u> to the <u style="color: brown;">yellow cabinet</u>, and place the <u style="color: orange;">bananas</u> on the <u>plate</u>.
          </div>
        </div>


        <div class="columns">
          <div class="column has-text-centered">
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/video_3.mp4" type="video/mp4">
            </video>
            <b>Positional Shift</b>: Place the <u style="color: green;">green pepper</u> in the <u>plate</u>, and the <u style="color: red;">red pepper</u> in the <u>plate</u>.
          </div>
          <div class="column has-text-centered">
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/video_4.mp4" type="video/mp4">
            </video>
            <b>Minor Adjustment</b>: Put the <u style="color: orange;">banana</u> onto the <u>plate</u> on the white table, and then put the <u style="color: blue;">blue dinosaur</u> from the box onto the <u>green mouse pad</u>.
          </div>
        </div>

        <div class="columns">
          <div class="column has-text-centered">
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/video_5.mp4" type="video/mp4">
            </video>
            <b>Appearance</b>: Move the <u style="color: yellow;">corn</u> to the <u>plate</u> in the cabinet, and put the <u style="color: green;">green pepper</u> onto the <u>plate</u> in the cabinet.
          </div>

          <div class="column has-text-centered">
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/video_6.mp4" type="video/mp4">
            </video>
            <b>Positional Shift</b>: Set the <u style="color: red;">red pepper</u> in the <u style="color: green;">green container</u>, and set the <u style="color: yellow;">corn</u> in the <u style="color: green;">green container</u>.
          </div>
        </div>


      </div>
  </section>


  <section class="section"  style="position: relative;">
    <div class="container is-max-widescreen">

      <div class="rows">


        <!-- Animation. -->
        <div class="rows is-centered ">
          <div class="row is-full-width">
            </br>
            </br>

            <h2 class="title is-3"><span class="dperact">Point Cloud to 3D Scene Graphs with Semantic Memory</span></h2>

            <!-- Interpolating. -->
            <div class="content has-text-justified">
              <!-- <br> -->
            </div>
            <div style="text-align: center;">
              <img src="media/figures/pointcloud_to_semanticMemory.png" class="interpolation-image" width="100%" height="100%" />
            </div>
            <!-- </br>
            </br> -->
            <p class="content has-text-justified">
              <b>DovSG</b> can accurately create 3D Scene Graphs that effectively guide robot navigation.
            </p>
          </div>
        </div>
  </section>


  <section class="section"  style="position: relative;">
    <div class="container is-max-widescreen">

      <div class="rows">


        <!-- Animation. -->
        <div class="rows is-centered ">
          <div class="row is-full-width">
            </br>
            </br>

            <h2 class="title is-3"><span class="dperact">Initialization and Construction of 3D Scene Graphs</span></h2>

            <!-- Interpolating. -->
            <div class="content has-text-justified">
              <!-- <br> -->
            </div>
            <div style="text-align: center;">
              <img src="media/figures/3d_scene_graph_generation.png" class="interpolation-image" width="80%" height="100%" />
            </div>
            <!-- </br>
            </br> -->
            <p class="content has-text-justified">
              We first use the RGB-D-based DROID-SLAM model to predict the pose of each frame in the scene. 
              Then, we apply an advanced Open-Vocal segmentation model to segment regions in the RGB images, 
              extract semantic feature vectors for each region, and project them onto a 3D point cloud. 
              Based on semantic, geometric, and CLIP feature similarities, the same object captured from multiple views is gradually associated and fused, 
              resulting in a series of 3D objects. Next, we infer the relationships between objects based on their spatial positions and generate edges connecting these objects, 
              forming a scene graph. This scene graph provides a structured and comprehensive understanding of the scene, 
              allowing efficient localization of target objects and enabling easy reconstruction and updating in dynamic environments, and supports task planning for large language models.
            </p>
          </div>
        </div>
  </section>


  <section class="section"  style="position: relative;">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
  
        <div class="column is-half">
          <h2 class="title is-3">
            <span class="dperact">Adaptation in interactions with manually modified scenes.</span>
          </h2>
          <div class="content has-text-justified">
          </div>
          <div style="text-align: center;">
            <img src="media/figures/relocalization_and_update.png" class="interpolation-image" width="80%" height="100%" />
          </div>
          <p class="content has-text-justified">
            <b>(1)</b> We train the scene-specific regression MLP of the ACE model using RGB images and their poses, making the process highly efficient.
            <b>(2)</b> After manual scene modification, multi-view observations allow rough global pose estimation via ACE, refined further using LightGlue and ICP.
            The new viewpointâ€™s point cloud closely aligns with the stored pose.
            <b>(3)</b> The bottom image shows accurate local updates to the scene based on observations from the new viewpoint.
          </p>
        </div>
  
        <div class="column is-half">
          <h2 class="title is-3">
            <span class="dperact">Two proposed grasp strategies in DovSG.</span>
          </h2>
          <div class="content has-text-justified">
          </div>
          <div style="text-align: center;">
            <img src="media/figures/pickup_strategy.png" class="interpolation-image" width="80%" height="100%" />
          </div>
          <p class="content has-text-justified">
            In the first row, we cropped the point cloud input into anyGrasp within a certain range around the target object, allowing anyGrasp to focus more on the target object without compromising the generation of collision-free grasps.
            Furthermore, we filtered the grasps based on translational and rotational costs, with the red grasps indicating the highest confidence.
            In the second row, we show our heuristic grasp strategy, which leverages the object's bounding box information to rotate and select the most appropriate grasp.
          </p>
        </div>
  
      </div>
    </div>
  </section>


  <section class="section"  style="position: relative;">
    <div class="container is-max-widescreen">

      <div class="rows">


        <!-- Animation. -->
        <div class="rows is-centered ">
          <div class="row is-full-width">
            </br>
            </br>

            <h2 class="title is-3"><span class="dperact">Detailed Relocalization Pipeline.</span></h2>

            <!-- Interpolating. -->
            <div class="content has-text-justified">
              <!-- <br> -->
            </div>
            <div style="text-align: center;">
              <img src="media/figures/detail_relocalization.png" class="interpolation-image" width="80%" height="100%" />
            </div>
            <!-- </br>
            </br> -->
            <p class="content has-text-justified">
              <b>(a)</b> The robot captures multiple images from various angles to obtain a wide field of view. These images include RGB images and depth maps (RGBDs), with each frame corresponding to its pose in the robot's base coordinate system. 
              <b>(b)</b> Since the pose information for each frame is known in the robot's base coordinate system, the relative poses between frames within the robot's coordinate system can be accurately calculated. 
              This provides important geometric constraints for the subsequent relocalization steps.
              <b>(c)</b> Using the LightGlue feature matching algorithm, the newly collected RGBDs are matched with the image data retained from the previous scene scanning phase. 
              From the matched historical frames, the RGBDs most similar to the newly collected data and their corresponding poses in the world coordinate system are extracted. 
              <b>(d)</b> Based on the ACE model, the rough pose of each newly observed frame in the world coordinate system is estimated, and each estimate is assigned a confidence score. 
              The pose with the highest confidence is then selected to transform the new observation point cloud into the world coordinate system. 
              <b>(e)</b> The historical image information associated with the new observation data (matched via LightGlue) is projected into the 3D point cloud within the world coordinate system, generating the target point cloud. 
              <b>(f)</b> Using the transformed point cloud from step (d) as the source point cloud and the target point cloud from step (e) as the reference, 
              the Iterative Closest Point (ICP) algorithm is employed for precise registration of the two. After ICP optimization, the source point cloud is accurately aligned with the world coordinate system. 
              <b>(g)</b> Based on the above process, the pose of the new observation point cloud is accurately estimated. 
              Figure (g) shows the point cloud result after relocalization, where the new observation point cloud successfully integrates into the global map with precise positioning.
            </p>
          </div>
        </div>
  </section>


  <section class="section"  style="position: relative;">
    <div class="container is-max-widescreen">

      <div class="rows">


        <!-- Animation. -->
        <div class="rows is-centered ">
          <div class="row is-full-width">
            </br>
            </br>

            <h2 class="title is-3"><span class="dperact">We tested the effectiveness of DovSG in four rooms of varying sizes and complexity.</span></h2>

            <!-- Interpolating. -->
            <div class="content has-text-justified">
              <!-- <br> -->
            </div>
            <div style="text-align: center;">
              <img src="media/figures/rooms.jpg" class="interpolation-image" width="50%" height="100%" />
            </div>
            <!-- </br>
            </br> -->
            <p class="content has-text-justified">
              We tested the effectiveness of DovSG in four rooms of varying sizes and complexity: 
              <b>Room 1</b> is approximately <b>60 mÂ²</b>, <b>Room 2</b> is about <b>40 mÂ²</b>, 
              <b>Room 3</b> is around <b>100 mÂ²</b>, and <b>Room 4</b> is about <b>150 mÂ²</b>. 
              <b>Rooms 1, 2, and 4</b> have relatively <b style="color: red;">complex scenes</b>, 
              while <b>Room 3</b> is <b style="color: green;">simpler</b>.
            </p>
          </div>
        </div>
  </section>



  <section class="section"  style="position: relative;">
    <div class="container is-max-widescreen">

      <div class="rows">


        <!-- Animation. -->
        <div class="rows is-centered ">
          <div class="row is-full-width">
            </br>
            </br>

            <h2 class="title is-3"><span class="dperact">The Success Rate of Long-term Tasks and SubTasks.</span></h2>

            <!-- Interpolating. -->
            <div class="content has-text-justified">
              <!-- <br> -->
            </div>
            <div style="text-align: center;">
              <img src="media/figures/success_rate_table.png" class="interpolation-image" width="80%" height="100%" />
            </div>
            <!-- </br>
            </br> -->
            <p class="content has-text-justified">
              We compared with <a href="https://ok-robot.github.io/">OK-Robot</a>, and the success rates of the experimental subtasks and long-term tasks are shown in the table above.
            </p>
          </div>
        </div>
  </section>




  <section class="section" style="position: relative;">
    <div class="container is-max-widescreen">
      <h2 class="title is-3"><span class="dperact">Youtube Video</span></h2>
      <div class="rows">
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/xmUCHzE6EYc" frameborder="0" allow="autoplay; encrypted-media"
            allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </section>



  <section class="section" id="BibTeX" style="position: relative;">
    <div class="container is-max-widescreen content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{yan2024dynamicopenvocabulary3dscene,
        title={Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation}, 
        author={Zhijie Yan and Shufei Li and Zuoxu Wang and Lixiu Wu and Han Wang and Jun Zhu and Lijiang Chen and Jihong Liu},
        year={2024},
        eprint={2410.11989},
        archivePrefix={arXiv},
        primaryClass={cs.RO},
        url={https://arxiv.org/abs/2410.11989}, 
  }</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-centered">
            <p>
              Send feedback and questions to <a href="mailto:yanzhijie@buaa.edu.cn">Zhijie Yan</a>. 
              Last updated on January 14, 2025.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>


</body>

</html>
